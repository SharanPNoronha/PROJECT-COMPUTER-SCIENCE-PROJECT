{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c822a4",
   "metadata": {},
   "source": [
    "# **Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d573a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow keras opencv-python scikit-learn matplotlib nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7f52b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdc2f2",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for i in np.linspace(0, length-1, num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def load_video_data(video_dir, labels_file, num_frames=10):\n",
    "    labels = {}\n",
    "    with open(labels_file, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            video, label = line.strip().split(',')\n",
    "            labels[video] = int(label)\n",
    "\n",
    "    X, y = [], []\n",
    "    for video_file in os.listdir(video_dir):\n",
    "        if video_file.endswith('.mp4'):\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            frames = extract_frames(video_path, num_frames)\n",
    "            X.append(frames)\n",
    "            y.append(labels[video_file])\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434b15c",
   "metadata": {},
   "source": [
    "#### Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017623a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, vocab_size=5000, sequence_length=100):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    token_to_id = {token: idx+1 for idx, token in enumerate(tokens[:vocab_size-1])}\n",
    "    sequences = [[token_to_id.get(token, 0) for token in tokens]]\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=sequence_length)\n",
    "    return padded_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec347ea",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec577c",
   "metadata": {},
   "source": [
    "#### NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nlp_model(vocab_size, embedding_dim, sequence_length):\n",
    "    input_text = Input(shape=(sequence_length,))\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "    lstm = LSTM(units=128, return_sequences=False)(embedding)\n",
    "    dropout = Dropout(0.5)(lstm)\n",
    "    output = Dense(1, activation='sigmoid')(dropout)\n",
    "    model = Model(inputs=input_text, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a89959",
   "metadata": {},
   "source": [
    "#### Facial Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278dd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_facial_recognition_model(input_shape):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    conv1 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_img)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    flatten = Flatten()(pool2)\n",
    "    dense = Dense(units=128, activation='relu')(flatten)\n",
    "    dropout = Dropout(0.5)(dense)\n",
    "    output = Dense(1, activation='sigmoid')(dropout)\n",
    "    model = Model(inputs=input_img, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136211ea",
   "metadata": {},
   "source": [
    "### Integrate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e7d29",
   "metadata": {},
   "source": [
    "#### Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_model(nlp_model, facial_model, sequence_length, input_shape):\n",
    "    combined_input = Input(shape=(sequence_length + np.prod(input_shape),))\n",
    "    \n",
    "    # NLP branch\n",
    "    nlp_input = tf.keras.layers.Lambda(lambda x: x[:, :sequence_length])(combined_input)\n",
    "    nlp_output = nlp_model(nlp_input)\n",
    "    \n",
    "    # Facial recognition branch\n",
    "    facial_input = tf.keras.layers.Lambda(lambda x: x[:, sequence_length:])(combined_input)\n",
    "    facial_reshaped = tf.keras.layers.Reshape(input_shape)(facial_input)\n",
    "    facial_output = facial_model(facial_reshaped)\n",
    "    \n",
    "    combined_output = tf.keras.layers.Concatenate()([nlp_output, facial_output])\n",
    "    final_output = Dense(1, activation='sigmoid')(combined_output)\n",
    "    \n",
    "    combined_model = Model(inputs=combined_input, outputs=final_output)\n",
    "    return combined_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616df245",
   "metadata": {},
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd548ff9",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example hyperparameters\n",
    "vocab_size = 5000\n",
    "embedding_dim = 128\n",
    "sequence_length = 100\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Build models\n",
    "nlp_model = build_nlp_model(vocab_size, embedding_dim, sequence_length)\n",
    "facial_model = build_facial_recognition_model(input_shape)\n",
    "combined_model = build_combined_model(nlp_model, facial_model, sequence_length, input_shape)\n",
    "\n",
    "# Compile combined model\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load and preprocess data\n",
    "# Note: Update paths to your dataset\n",
    "video_dir = 'path_to_video_data'\n",
    "labels_file = 'path_to_labels.csv'\n",
    "X_videos, y = load_video_data(video_dir, labels_file)\n",
    "\n",
    "# Assuming text data is preprocessed similarly\n",
    "X_text = np.array([preprocess_text(\"example text data\", vocab_size, sequence_length) for _ in range(len(y))])\n",
    "\n",
    "# Combine video frames and text sequences\n",
    "X_combined = np.concatenate([X_text.reshape(len(X_text), -1), X_videos.reshape(len(X_videos), -1)], axis=1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = combined_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e19f2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = combined_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d49e96",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training & validation accuracy and loss\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
